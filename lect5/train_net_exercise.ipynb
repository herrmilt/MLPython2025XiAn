{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875ca8bc",
   "metadata": {},
   "source": [
    "1. In this model for 2D keypoint detection, how many keypoints are being detected per image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23127726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, \n",
    "                               kernel_size=6, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3, \n",
    "                               padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.fc = nn.Linear(12 * 2 * 2, 2)  \n",
    "        self.tanh = nn.Tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2352bed6",
   "metadata": {},
   "source": [
    "2. And this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareKPNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SquareKPNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, \n",
    "                               kernel_size=6, padding=1, device=device)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3, \n",
    "                               padding=1, device=device)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.fc = nn.Linear(12 * 2 * 2, 8, device=device)  # Adjust the dimensions accordingly\n",
    "        self.tanh = nn.Tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32bebae",
   "metadata": {},
   "source": [
    "3. For single object detection, what is this CustomLoss meassuring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2302563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        pred_is_object = predictions[:, 0]\n",
    "        pred_box = predictions[:, 1:5]\n",
    "        pred_logits = predictions[:, 5:7]\n",
    "\n",
    "        target_is_object = targets[:, 0]\n",
    "        target_box = targets[:, 1:5]\n",
    "        target_class = targets[:, 5].to(torch.long)\n",
    "\n",
    "        loss = F.mse_loss(pred_is_object, target_is_object)\n",
    "        is_object = target_is_object == 1\n",
    "        \n",
    "        if torch.any(is_object):\n",
    "            loss += F.mse_loss(pred_box[is_object], target_box[is_object])\n",
    "            loss += F.cross_entropy(pred_logits[is_object], target_class[is_object])\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09558981",
   "metadata": {},
   "source": [
    "4. In this model, how is the image size reduced without a pooling layer?\n",
    "    - What is latent_dim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6636527",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # 28x28 -> 14x14\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # 14x14 -> 7x7\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 7 * 7, latent_dim),  # Latent representation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0b7f5",
   "metadata": {},
   "source": [
    "5. In this model, what is the Unflatten layer used for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27419b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = nn.Sequential(\n",
    "    nn.Linear(latent_dim, 32 * 7 * 7),\n",
    "    nn.Unflatten(1, (32, 7, 7)),\n",
    "    nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # 7x7 -> 14x14\n",
    "    nn.LeakyReLU(),\n",
    "    nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),  # 14x14 -> 28x28\n",
    "    nn.Sigmoid()  # Output pixel values in range [0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def5034",
   "metadata": {},
   "source": [
    "6. What is this piece of coding doing?\n",
    "    - What is the unsqueeze() method in line 6 used for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59859c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    x, _ = dataset[i]\n",
    "    noise = torch.randn_like(x) * 0.1\n",
    "    x = x + noise\n",
    "    with torch.no_grad():\n",
    "        image = model(x.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a266b2",
   "metadata": {},
   "source": [
    "7. In this code, what is fc_mu and fc_logvar layers used for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5cc715",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)  # 28x28 -> 14x14\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)  # 14x14 -> 7x7\n",
    "        self.fc_mu = nn.Linear(32 * 7 * 7, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(32 * 7 * 7, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_mu = self.fc_mu(x)\n",
    "        x_logvar = self.fc_logvar(x)\n",
    "        return x_mu, x_logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebafcec7",
   "metadata": {},
   "source": [
    "8. In this VAEDecoder, what is the self.fc layer used for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ab719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 32 * 7 * 7)\n",
    "        self.conv2 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1)  # 7x7 -> 14x14\n",
    "        self.conv1 = nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1)  # 14x14 -> 28x28\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), 32, 7, 7)\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a3dfd",
   "metadata": {},
   "source": [
    "9. This code comes from a GAN network.\n",
    "    - Why are there three different loss calculated?\n",
    "    - Which GAN component is learning on each code fragment?\n",
    "    - Why is fake.detach() in line 14?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c5ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### with real data\n",
    "real_images = data.to(device)\n",
    "batch_size = real_images.size(0)\n",
    "label = torch.full((batch_size,), REAL_LABEL, dtype=torch.float, device=device)\n",
    "output = model_D(real_images).view(-1)\n",
    "loss_real = loss_fn(output, label)\n",
    "loss_real.backward()\n",
    "conf_D_real = output.mean().item()\n",
    "\n",
    "### with fake data\n",
    "noise = torch.randn(batch_size, NOISE_SIZE, 1, 1, device=device)\n",
    "fake = model_G(noise)\n",
    "label.fill_(FAKE_LABEL)\n",
    "output = model_D(fake.detach()).view(-1)\n",
    "loss_fake = loss_fn(output, label)\n",
    "loss_fake.backward()\n",
    "conf_D_fake = 1 - output.mean().item()\n",
    "loss_D = loss_real + loss_fake\n",
    "optimizerD.step()\n",
    "\n",
    "# Update the generator\n",
    "model_G.zero_grad()\n",
    "label.fill_(REAL_LABEL)  \n",
    "output = model_D(fake).view(-1)\n",
    "loss_G = loss_fn(output, label)\n",
    "loss_G.backward()\n",
    "optimizerG.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc98221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d306a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde77ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308842d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734276fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
