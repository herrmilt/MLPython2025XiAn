{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3933878",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "## Weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5012411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a simple neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# Step 3: Set up loss and optimizer WITH weight decay\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, \n",
    "                       weight_decay=1e-4)  # Weight decay here!\n",
    "\n",
    "# Step 4: Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X).squeeze()\n",
    "        loss = criterion(outputs, batch_y.float())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d853e",
   "metadata": {},
   "source": [
    "## Dataset augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In images\n",
    "\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 50% chance to flip\n",
    "    transforms.RandomRotation(degrees=15),   # Rotate Â±15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),  # Random crop + resize\n",
    "    transforms.ToTensor(),  # Convert to tensor (normalizes to [0,1])\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, \n",
    "                           download=True, transform=augmentation)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "for images, labels in dataloader:\n",
    "\n",
    "    # forward pass\n",
    "    output = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In text processing\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Synonym replacement\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "augmented_text = aug.augment(text)\n",
    "print(augmented_text)  # e.g., \"The fast brown fox leaps over the lazy dog\"\n",
    "\n",
    "# Random word swap\n",
    "aug = naw.RandomWordAug(action=\"swap\")\n",
    "print(aug.augment(text))  # e.g., \"The brown quick fox jumps lazy the over dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In time series\n",
    "\n",
    "import numpy as np\n",
    "from tsaug import TimeWarp, AddNoise, Drift\n",
    "\n",
    "# Sample time-series (e.g., sensor data)\n",
    "X = np.random.randn(100, 3)  # 100 timesteps, 3 features\n",
    "\n",
    "# Apply augmentations\n",
    "augmented = (\n",
    "    TimeWarp(n_speed_change=3)  # Random speed changes\n",
    "    + AddNoise(scale=0.1)       # Add Gaussian noise\n",
    "    + Drift(max_drift=0.3)      # Simulate sensor drift\n",
    ").augment(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5646ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In tabular data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Sample tabular data\n",
    "X = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]  # Features\n",
    "y = [0, 1, 0]                              # Imbalanced labels\n",
    "\n",
    "# Generate synthetic samples\n",
    "smote = SMOTE()\n",
    "X_aug, y_aug = smote.fit_resample(X, y)\n",
    "print(X_aug)  # e.g., [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6], [0.35, 0.45]] (new sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fa1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In audio data\n",
    "import librosa\n",
    "\n",
    "# Load audio\n",
    "y, sr = librosa.load(\"audio.wav\")\n",
    "\n",
    "# Pitch shift\n",
    "y_shifted = librosa.effects.pitch_shift(y, sr, n_steps=2)  # Shift up 2 semitones\n",
    "\n",
    "# Time stretch\n",
    "y_stretched = librosa.effects.time_stretch(y, rate=1.2)  # 20% slower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8442ac1",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf9ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(10, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).squeeze()\n",
    "\n",
    "model = Classifier()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def should_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss - self.min_delta:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "early_stopper = EarlyStopper(patience=5, min_delta=0.01)\n",
    "best_model_weights = None\n",
    "\n",
    "for epoch in range(100):  # Max epochs\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = validate(model, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss/len(train_loader):.4f}, Val Loss = {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopper.should_stop(val_loss):\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "        break\n",
    "\n",
    "    # Optional: Save best model weights\n",
    "    if val_loss == early_stopper.min_validation_loss:\n",
    "        best_model_weights = model.state_dict().copy()\n",
    "\n",
    "# Restore best model (optional)\n",
    "if best_model_weights:\n",
    "    model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988849f",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetNoDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "\n",
    "class NetWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Dropout applied after activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.fc3(x)\n",
    "    \n",
    "\n",
    "# model.train()\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face8594",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "\n",
    "Batch Normalization is like a traffic cop for your neural network:\n",
    "\n",
    "    - Problem: Without it, some neurons get \"loud\" (huge numbers) while others stay \"quiet\" (tiny decimals), making training chaotic.\n",
    "\n",
    "    - Solution:\n",
    "\n",
    "        - Normalize: Every batch of data is adjusted to have an average of 0 and a standard deviation of 1 (like grading on a curve).\n",
    "\n",
    "        - Scale & Shift: Then, the network learns how much to \"re-adjust\" the values (like turning up/down the volume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22146ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetNoBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(20, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "class NetWithBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(20, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)  # BatchNorm after first layer\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)  # BatchNorm after second layer\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "# model.train()\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451f834",
   "metadata": {},
   "source": [
    "# Adversarial attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e97c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "\n",
    "def fgsm_attack(model, x, y, epsilon, loss_fn):\n",
    "    x.requires_grad = True\n",
    "    outputs = model(x)\n",
    "    loss = loss_fn(outputs, y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    perturbed_x = x + epsilon * x.grad.sign()\n",
    "    return torch.clamp(perturbed_x, 0, 1)  # For image data\n",
    "\n",
    "def adversarial_train(model, train_loader, epsilon=0.01, epochs=10):\n",
    "    optimizer = SGD(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            # Generate adversarial examples\n",
    "            x_adv = fgsm_attack(model, x, y, epsilon, loss_fn)\n",
    "            # Combined loss (clean + adversarial)\n",
    "            outputs_clean = model(x)\n",
    "            outputs_adv = model(x_adv)\n",
    "            loss = 0.5 * (loss_fn(outputs_clean, y) + 0.5 * loss_fn(outputs_adv, y))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
